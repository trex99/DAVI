# -*- coding: utf-8 -*-
"""[문제]중간고사_미완성_문제해결_프로그램_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OYYL4rVqDIm9IkdmcnG92nGxVCDqzBHQ

# 👍 중간고사 - 미완성 문제해결 프로그램

## [시험문제]
- 주석의 안내에 따라 미완성 프로그램을 완성하고, 중간고사 시험문제 16번 ~ 20번까지의 질문에 답하세요.

## [문제상황 - 해결할 문제 정의]

- 네이버 검색엔진을 이용하여 “데이터 분석”을 키워드로 하여 검색한 결과를 
- 제목, 상세 경로 등 여러가지 정보를 수집하여 파일로 저장하는 스크래핑 프로그램을 완성하세요.

- 접속 사이트 : https://search.naver.com/search.naver?where=m_view&sm=mtb_jum&query=

- 수집 대상 : '데이터 분석' 키워드 조회결과

# 조회결과 스크래핑
"""

# 필요한 모듈 로딩
import ? # 문제 2-1 : pandas 패키지를 pd라는 이름으로 사용하기 위한 코드를 완성하세요.
import ? # 문제 2-2 : numpy 패키지를 np라는 이름으로 사용하기 위한 코드를 완성하세요.
import re

import requests
from bs4 import BeautifulSoup
import csv

import datetime

# "데이터 분석" 키워드를 이용하여 네이버 검색결과인 원시 데이터를 수집
key_word = '데이터 분석'
url = f'https://m.search.naver.com/search.naver?where=m_view&sm=mtb_jum&query={key_word}'
html = requests.get(url)
soup = ?  # 문제 2-3 : 수신한 데이터(html)를 구문 분석하여 접근하기 쉽도록 처리하는 코드를 완성하세요.

# 일단 원시 데이터를 보조 기억장치에 저장한다.
# 하단의 프로그램을 실행하는 도중 문제 발생시 다시 검색하는 번거로움과 시간을 절약할 수 있다.
f = open('search_result_source.txt', 'w')
f.write(html.content.decode('utf-8'))

# 저장된 검색 결과 데이터 파일을 읽기
f = open('search_result_source.txt')
html = f.?()  # 문제 2-4 : 파일에서 데이터를 모두 읽는 함수를 완성하세요.
soup = BeautifulSoup(html, 'html.parser')

# "bx _svp_item _slog_visible" CSS를 사용하는 Tag를 찾는다.
search_result_cards = soup.?('.bx._svp_item._slog_visible') # 문제 2-5 : Tag를 찾는 알맞은 함수를 완성하세요.

len(search_result_cards)

# 찾은 Tag 중 첫번째 데이터를 출력해 본다.
search_result_cards[0]

# 수집 대상이 되는 출처URL, 출처명, 작성일자, 제목, 제목클릭시 이동 URL, 제목클릭시 실행함수, 요약 내용 데이터를 수집한다.
# 수집된 데이터는 data_list 객체에 저장한다.
data_list = []
for card in search_result_cards:
    temp = []

    # 출처 URL 수집
    try:
        card_source = card.select('.elss.etc_dsc_inner')[0].a
    except:
        card_source = card.select('.source_link')[0]
    # 문제 2-6 : 출처 URL을 수집하기 위한 알맞은 이름을 완성하세요. (이미 코드가 완성되어서 시험문제로 제출하지 않습니다.)
    temp.append(card_source['href']) # 출처 URL

    # 출처명 수집
    try:
        card_source = card.select('.elss.etc_dsc_inner')[0].a
    except:
        card_source = card.select('.source_txt.name')[0]
    # 문제 2-7 : 출처명을 수집하기 위한 알맞은 변수명을 완성하세요.
    temp.append(card_source.?)  # 출처명

    # 작성일자 수집
    try:
        card_date = card.select('.sub_time.sub_txt')[0]
    except:
        card_date = card.select('.source_txt.date')[0]
    # 문제 2-8 : 작성일자를 수집하기 위한 알맞은 변수명을 완성하세요.
    temp.append(card_date.?) # 작성일자

    # 제목, 제목클릭시 이동 URL, 제목클릭시 실행함수 수집
    card_title = card.select('.api_txt_lines.total_tit')[0]
    try:
        temp.append(card_title['href']) # 제목 클릭시 이동하는 URL
        temp.append(card_title.attrs['onclick']) # 제목 클릭시 실행하는 함수
        temp.append(card_title.text) # 제목
    except:
        temp.append(np.NaN) # 제목 클릭시 이동하는 URL 결측치 처리
        temp.append(np.NaN) # 제목 클릭시 실행하는 함수 결측치 처리
        temp.append(np.NaN) # 제목 결측치 처리

    # 요약 내용 수집
    card_contents = card.select('.api_txt_lines.dsc_txt')[0]
    temp.append(card_contents.text) # 요약 내용

    data_list.append(temp)

# 수집된 데이터가 저장된 data_list를 DataFrame객체로 변환한다.
# 문제 2-9 : pandas 데이터프레임으로 변환하기 위한 함수를 완성하세요.
df_data_list = pd.?(data_list, columns=['출처 URL', 
                                                '출처명', 
                                                '작성일자', 
                                                '제목 클릭 이동 URL', 
                                                '제목 클릭 실행 함수', 
                                                '제목',
                                                '요약내용'])

# 변환된 df_data_list객체를 출력해 본다.
df_data_list

# 25번째 Row만 확인한다. "작성일자" 컬럼 값이 "6일 전"으로 되어있다.
# 문제 2-10 : 25번째 Row를 확인하기 위한 코드를 완성하세요. (이미 코드가 완성되지 않아서 시험문제로 제출되지 않습니다.)
df_data_list.iloc[25]

# "작성일자" 컬럼의 값은 날짜형식이기 때문에 일관성을 갖도록 숫자를 제외한 모든 문자를 제거한다.
# 문자열에서 숫자만을 선택하는 함수 제작
def get_number(x):
    # 만일을 위해, x 값이 숫자형 데이터인 경우 오류가 발생하지 않도록 문자열 데이터로 변환한다.
    x = str(x)
    # x 값에서 공백을 제거한다.
    x = x.replace(' ', '')

    # x 값이 "n일전"과 같은 표현의 경우 현재날짜에서 n일을 빼서 날짜를 만든다.
    if '일전' in x:  # x 문자열에 '일정'이라는 문자를 포함한다면
        n = x.replace('일전', '')  # '일전' 문자를 제거하여 숫자만 남도록 한다.
        n = int(n)  # 숫자만 남았으나 숫자형 변수로 변환한다.
        today = datetime.datetime.now() # 오늘 날짜, 시간을 구한다.
        today_n_ago = today - datetime.timedelta(days=n) # 오늘 날짜에서 n일을 뺀다.
        numbers = today_n_ago.strftime('%Y%m%d') # 날짜 자료형을 "YYYYMMDD"형식으로 변환한다.
    else: # x 문자열에 '일정'이라는 문자를 포함하지 않는다면
        numbers = re.sub('[^0-9]', '', str(x)) # 문자열에서 숫자를 제외한 모든 문자를 제거한다.
    return numbers

# 문자열에서 숫자만을 선택하는 함수를 이용하여 각 Row의 "작성일자" 컬럼값에서 숫자만 가져와서 업데이트한다.
df_data_list['작성일자'] = df_data_list['작성일자'].apply(get_number)

# 25번째 Row만 확인한다. "작성일자" 컬럼 값이 "6일 전"이 아닌 현재 날짜 이전 6일로 되어있다.
# 문제 2-10 : 25번째 Row를 확인하기 위한 코드를 완성하세요. (이미 코드가 완성되지 않아서 시험문제로 제출되지 않습니다.)
df_data_list.iloc[25]

# 수집된 데이터의 "작성일자"가 언제부터 ~ 언제까지인지 조사한다.
# 문제 2-11 : 작성일자 컬럼의 최대값을 구하는 함수를 완성하세요.
# 문제 2-12 : 작성일자 컬럼의 최소값을 구하는 함수를 완성하세요.
df_data_list['작성일자'].?(), df_data_list['작성일자'].?()

# DataFrame객체를 저장한다.
# 단, 자동 생성된 index는 필요하지 않기 때문에 제외한다.
# 문제 2-13 : 데이터프레임 객체를 저장하기 위한 함수를 완성하세요.
df_data_list.?('search_result_source.csv', index=False)

# 저장된 파일목록을 확인한다.
import glob
glob.glob('./**')

# 저장된 파일의 내용을 확인한다.
# 문제 2-14 : 저장된 CSV파일을 읽기 위한 코드를 완성하세요.
df = pd.?('search_result_source.csv')
df

"""- End Of Program"""